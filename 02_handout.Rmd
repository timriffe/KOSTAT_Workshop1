---
title: | 
  | \includegraphics{logotip.pdf}
  |
  | KOSTAT-UNFPA Summer Seminar on Population
  | \vspace{1.5cm} \LARGE \emph{Workshop~1.~Demography in R}
  | \vspace{0.3cm} \huge \textbf{Day 2: The tidy data approach}\vspace{0.6cm}
  | 
fontsize: 11pt
geometry: a4paper, twoside, left=2.5cm, right=2.5cm, top=2cm, bottom=2.8cm, headsep
  = 1.35cm, footskip = 1.6cm
output:
  pdf_document:
    number_sections: yes
  html_document2: default
  html_document:
    number_sections: yes
    toc: yes
  pdf_document2: default
  header-includes:
    - \usepackage{titling}
    - \usepackage{fancyhdr}
    - \pagestyle{fancy}
    - \fancyhead[LE]{\thepage~\qquad~KOSTAT-UNFPA Summer Seminar on Population}
    - \fancyhead[RE]{Workshop~1.~Demography in R}
    - \fancyhead[LO]{{Day 2: The tidy data approach}}
    - \fancyhead[RO]{Tim Riffe\qquad~\thepage}
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(pw)
rm(us)
```

\noindent\makebox[\textwidth][c]{
  \begin{minipage}[t]{0.45\textwidth}
    \centering
    \Large{Instructor: Tim Riffe} \\
    \vspace{0.1cm}\large{\texttt{tim.riffe@gmail.com}}
   
    \vspace{.5cm}
    \Large{Assistant: Rustam Tursun-Zade}
    \vspace{0.1cm}\large{\texttt{rustam.tursunzade@gmail.com}}
  \end{minipage}
}


\vspace{0.8cm}
\begin{center}
\large{27 July 2021}
\end{center}
\vspace{0.8cm}


\tableofcontents

# Tidy data


## Definition
Tidy data follows a standard structure where each column is a variable, each row is an observation, and each cell is a value. Anything else is messy. It's literally that straightforward. A more complete definition can be found here: [https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) Demographic data is often delivered in a tidy format. When it is not, then it can be reshaped into a tidy format.

Tidyverse packages work well together because they share a standard approach to formatting and working with datasets. Tidy datasets processed using tidyverse tools allow for fast and understandable analyses that in many cases require no *programming*, whereas it often takes a certain amount of head-scratching (programming) to analyze not-tidy datasets. 

Tidy datasets can also be visualized without further ado using a systematic grammar [@wilkinson2012grammar] implemented in the `ggplot2` package ( @wickham2016ggplot2, this loads automatically with `tidyverse`). Today we will do just basic examples, but this will be made more explicit on Thursday.

## Example of not-tidy data

 The following layout (screenshot from Excel) is not tidy. This example data was manually extracted from here: [https://appsso.eurostat.ec.europa.eu/nui/show.do?dataset=demo_fasec&lang=en](https://appsso.eurostat.ec.europa.eu/nui/show.do?dataset=demo_fasec&lang=en), and the given format was concocted with EUROSTAT's online data widget.

\begin{center}
\includegraphics[]{demo_fasec_screenshot.png}
\end{center}

The main thing that makes it not-tidy are the years spread over columns. These should be stacked into to columns: `TIME` (per the original codes) and `Births`, which are the values in the cells. The fact that `AGE` is coded as an arithmetically unusable character string is something we'll want to recode, but it is orthogonal to the *tidiness* of the data. Finally, we will ensure that age-specific births sum up to the stated total births per year and country. 

To follow along, create a folder in your project called `Data`. Then, go to the `Data` folder of the course repository on github:
[https://github.com/timriffe/KOSTAT_Workshop1/blob/master/Data/demo_fasec.xlsx](https://github.com/timriffe/KOSTAT_Workshop1/blob/master/Data/demo_fasec.xlsx) and click `Download`. Move it to the data folder you just made. You can also do the same for a second file that we'll use later today:
[https://github.com/timriffe/KOSTAT_Workshop1/blob/master/Data/demo_pjan.xlsx](https://github.com/timriffe/KOSTAT_Workshop1/blob/master/Data/demo_pjan.xlsx)

# Introducing `tidyverse`: a worked example

Today's entire session will be working with this smallish births dataset.

## Reshape, Rescale, Recode it 

We'll use the `read_excel()` function from the `readxl` package to get the data in. First let's look at the help file using `?read_excel`. Visual inspection of the data shows us that we need to skip several rows, plus there's a note at the bottom of the sheet that we want to ignore. We specify an explicit cell range using the argument `range` and giving spreadsheet coordinates `range = "A10:H158"`

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(readxl)
?read_excel
Wide <- read_excel(
           path = "Data/demo_fasec.xlsx",
           range = "A10:H158")
glimpse(Wide)
```

### `pivot_longer()` (`pivot_wider()`)
These data are not tidy because `TIME` is spread over columns. Instead we should have a column called `TIME`, containing the years, and the cell values currently in the columns `2011` to `2016` should be in a column called births. The function `pivot_longer()` will do this for us. See `?pivot_longer`.

```{r}
Long <- pivot_longer(
          data = Wide, 
          cols = `2011`:`2016`,
          names_to = "TIME",
          values_to = "Births")

glimpse(Long)
```

The opposite of `pivot_longer()` is `pivot_wider()`. If ever you have the need to go back in the other direction, you can do so like this:

```{r}
# reverse operation
pivot_wider(data = Long,
            names_from = "TIME",
            values_from = "Births")
```

Pivoting between columns and rows has some more options that we will surely make use of as this course progresses. The arguments `names_to` (`names_from`) and `values_to` (`values_from`) are designed to be easy to understand and remember. A nice thing about `pivot_longer` is you can specify column ranges using names (see how we put back-ticks on years? That's so that 2011 doesn't get interpreted at the 2011th column!). Integer ranges will also do, as would listing out the columns by name or position. There are other tricks for intelligently picking out columns, see `?tidyr_tidy_select`. From that help file, we see other things would have worked in our case, for example `where(is.double)`, which is handy.

```{r, eval = FALSE}
pivot_longer(
          data = Wide, 
          cols = where(is.double),
          names_to = "TIME",
          values_to = "Births")
```

### `select()` columns

Note, some of these columns are clean and ready to use (`TIME` and `Births`), but `GEO/TIME` is not a useful column name, and the values in `AGE` might not be useful in practice. Instead, maybe we want `AGE` to be an integer value so that we can sort on it. Maybe also we don't want to have a `Total` value for `AGE`, but instead we want to ensure that the age-specific births add up to the total? This will give us a chance to showcase some more tidyverse tools:

Select, rearrange, and rename **columns** using `select()`. Notice that i) the left side of each `=` is the new column name, ii) the order you list the columns is the new order, iii) if you forget to list a column you lose it! iv) when in doubt, but back-ticks on the column name to ensure it won't be misinterpreted. `GEO/TIME` looks like math, so stick it in back-ticks...

```{r}
Long <- select(
  .data = Long,
  Country = `GEO/TIME`,
  Age = AGE,
  Year = TIME,
  Births 
)
head(Long)
```

### pipes, `%>%`

Now seems a natural-enough place to demonstrate piping. Pipes allow us to string together data operations into a single sequence to be executed at once. Thus far we have read in the data, reshaped it, and re-specified columns. All together, this becomes:

```{r}
Long <- 
  read_excel(
    path = "Data/demo_fasec.xlsx",
    range = "A10:H158") %>% 
  pivot_longer(
    cols = `2011`:`2016`,
    names_to = "Year",
    values_to = "Births") %>% 
  select(
     Country = `GEO/TIME`,
     Year,
     Age = AGE,
     Births)
```

This reads as "first `read_excel()`, then `pivot_longer()`, then `select()` columns". Notice how the functions can be read as verbs, and the the pipes allow them to be combined into a rote kind of sentence. Indeed, it can help to add notes using `#`: don't worry: it won't break the chain! As the dataset goes down the pipeline, by default it becomes the first argument to the next function to be executed. Each of these functions has a first argument called either `data` or `.data`, which doesn't need to be specified because the incoming data is passed to it. 

```{r}
Long <- 
  # first read in from Excel
  read_excel(
    path = "Data/demo_fasec.xlsx",
    range = "A10:H158") %>% 
  # stack years
  pivot_longer(
    cols = `2011`:`2016`,
    names_to = "Year",
    values_to = "Births") %>% 
  # pick out the columns
  select(
     Country = `GEO/TIME`,
     Year,
     Age = AGE,
     Births)
```

Note: you can run this code by simply placing the cursor anywhere in the pipeline and pressing `Ctrl + Enter`. There is no need to select the whole statement before running, although this also works (you could in this case also click the green play arrow).

We will augment this pipeline step by step and then recompose it in its entirety at the end.

### `filter()`, `mutate()`, `group_by()`

So what age classes to we have? `unique()` picks out just the unique values present in a vector.
```{r}
# selecting a column with $ 
unique(Long$Age)

# same thing using tidy:
Long %>% 
  pull(Age) %>% # pull() extracts column as vector
  unique()
```

The `Age` column should be changed to consist in just integers. But this raises another issue: what to do with the `Total` and `Unknown` ages? My preference is usually to redistribute unknowns proportional to the distribution of any *knowns*:

$$ \widehat{Y_x} = Y_x + Y_{unknown} * \frac{Y_x}{\sum Y_x}$$
where the denominator excludes unknowns... This is just the same as rescaling the distributions of *known* ages to sum to the stated total

$$ \widehat{Y_x} = Y * \frac{Y_x}{\sum Y_x}$$
(where $x$ excludes unknowns)

Once we do one or the other of these operations, we'll end up with just ages `15 years` through `49 years`, and can convert using string operators. We can throw out either `Total` or `Unknown` using `filter()` to select rows. Calculations to redistribute can be done using the function `mutate()`. The basic structure of said operation would be something similar to:

```{r, eval = FALSE}
# don't run this
Long %>% 
  # 1
  mutate(TOT = Births[Age == "Total"]) %>% 
  # 2
  filter(! Age %in% c("Total", "Unknown")) %>% 
  # 3
  mutate(Births = Births / sum(Births) * TOT)
```

I'll first explain the basic logic, then why it won't *yet* work as expected. In step 1, we use `mutate()` to create a new column called `TOT`, which just repeats the respective value for each row of the data. 

Now for the `filter()` statement.

### Time out for logicals

Each value of `TOT` is intended to be the value of `Births` where `Age` is equal to `"Total"`. Note `==` is a *logical* equals, meaning you're asking if values are equal. The result will be `TRUE`, `FALSE`, or `NA` if pertinent.

Example:
```{r}
1:5 == 5
```

Other useful logical operators include `!=` (inequality), `<`, `>`, `<=`, `>=`. Further logical functions include: `is.na()`, `any()`, `all()`. Each of these operators and functions is vectorized, meaning they can evaluate long vectors of expressions element-wise.

Here we want to use this logical vector to select values:

```{r}
abcde <- c("a","b","c","d","e")
abcde[1:5 == 5]
```
Namely, we get back the values where the logical vector evaluates to `TRUE`. 

Given a columns `TOT`, we can remove age classes equal to `"Total"` or `"Unknown"` with `filter()`. `%in%` is a logical operator for set membership.

```{r}
c("a","d","k") %in% abcde
```

Finally, `mutate()` can be used to do the rescale operation using our basic arithmetic. 

### `group_by()` (`ungroup()`)

An issue that you may foresee at this point is that either of the above formulas is independent within each `Country` and `Year`. We can deal with this by declaring each combination of these two variables as an *independent* group using `group_by()`, and then removing groups when no longer needed using `ungroup()`. That's just good housekeeping, but it keeps the pipeline rigorous: You can assume group declarations will persist until explicitly removed. 

```{r}
Long2 <-
  Long %>% 
  # add group metadata
  group_by(Country, Year) %>% 
  # raise Total count to column for element-wise rescale
  mutate(TOT = Births[Age == "Total"]) %>% 
  # throw out Total and Unknown ages
  filter(! Age %in% c("Total", "Unknown")) %>% 
  # rescale proportions known to stated total
  mutate(Births = Births / sum(Births) * TOT) %>% 
  # groups no longer needed, let's remove them:
  ungroup()
```

Finally, we can clean up the `Age` column! Here I'll take the string substitution strategy, although other options would also work. `gsub()` looks for a pattern in the string `" years"` and replaces it. In this case, I replace with an empty string `""`, so `"15 years"` becomes `"15"`, still a character string. We can then modify it in the same `mutate()` call: comma-separated statements in `mutate()` are evaluated in sequence, and they can be sequentially dependent!

```{r}
Long2 %>% 
  mutate(Age = gsub(Age,
                    pattern = " years", 
                    replacement = ""),
         Age = as.integer(Age))
```

Note, you can also use pipes inside function calls, like `mutate()`, so the above could become:

```{r}
Long2 %>% 
  mutate(Age = Age %>%  
                gsub(
                  pattern = " years", 
                  replacement = "") %>% 
               as.integer())
```

Depending on what you're doing, one or the other of these could be more *legible*. Human-legible code is more robust than illegible code, can we agree on this point?

### Bringing it all together

There are times when it may make sense to keep steps separate, in separate data objects, but our first example is a case of wanting to keep all steps contained in a single pipeline. That's because the intermediate pieces are redundant and add no value. Combined into a single pipeline, we'd end up with something like this:

```{r}
Births <- 
  # first read in from Excel
  read_excel(
    path = "Data/demo_fasec.xlsx",
    range = "A10:H158") %>% 
  # stack years
  pivot_longer(
    cols = `2011`:`2016`,
    names_to = "Year",
    values_to = "Births") %>% 
  # pick out the columns
  select(
     Country = `GEO/TIME`,
     Year,
     Age = AGE,
     Births) %>% 
  # add group metadata
  group_by(Country, Year) %>% 
  # raise Total count to column for element-wise rescale
  mutate(TOT = Births[Age == "Total"]) %>% 
  # throw out Total and Unknown ages
  filter(! Age %in% c("Total", "Unknown")) %>% 
  # rescale proportions known to stated total
  mutate(Births = Births / sum(Births) * TOT) %>% 
  # groups no longer needed, let's remove them:
  ungroup() %>% 
  # clean up Age
  mutate(Age = Age %>%  
                gsub(
                  pattern = " years", 
                  replacement = "") %>% 
                as.integer(),
         Year = as.integer(Year)) %>% 
  # sort rows
  arrange(Country, Year, Age) %>% 
  # remove TOT column, no longer needed
  select(-TOT)

# have a look
glimpse(Births)
```

This is a tidy pipeline. And tidy code, no matter who writes it, usually ends up looking something like this. To finish off the pipeline, I've sorted the rows. `arrange(Country, Year, Age)` sorts `Age` within `Year` within `Country`), and we delete the `TOT` column with subtraction inside `select()`. 

You see all those annotations between many of the pipe steps? That's not *just* for you, the reader. It's good practice to do that. Possibly because someone else might like to interpret your code, so why not make it easier, but also you should comment your code out of respect for *future you*, because *future you* won't remember what you were thinking when you wrote it.

## Aggregate with `summarize()`

Aggregation typically implies a reduction in the number of rows in a data set. Let's see examples of grouping countries, grouping ages, and calculating marginal sums.

### Group to 5-year ages

Grouping ages or years often follows a similar logic. We will exploit to the *modulo* operator, `%%`, which tells us the remainder after Euclidean division. Example:

```{r}
a <- 1:10
a %% 2
a %% 5
```
That is the divisor (2 or 5) is subtracted away an integer number of times until what remains is smaller than the divisor. This is useful for redefining `Age`, see:

```{r}
Age <- 0:20
Age - Age %% 5
```

That is, subtracting `Age` modulo 5 from a vector of single ages tells you the lower bound of the five year age group that each single age lays within. We can then use this new age vector to group data, and finally we aggregate `Births` using `summarize()`.

```{r}
Births %>% 
  mutate(Age = Age - Age %% 5) %>% 
  group_by(Country, Year, Age) %>% 
  summarize(Births = sum(Births),
            .groups = "drop")
```

`Births = sum(Births)` might look strange. The left side is a single outgoing row, whereas the right side is a vector with five values. Our dataset of 840 rows is in this way reduced to $840 / 5 = 168$ rows. This works out cleanly in our case because the age groups were cleanly divisible. Not the argument `.groups = "drop"` at the end of `summarize()`, this is just the same as adding `%>% ungroup()` at the end of the pipeline.

### Marginal sums

The result of a summary statement could be just a single row, in this case a probably not-useful result.

```{r}
Births %>% 
  summarize(Births = sum(Births))
```

To get totals by `Country` and `Year`, once again we apply groups:

```{r}
Births %>% 
  group_by(Country, Year) %>% 
  summarize(Births = sum(Births),
            .groups = "drop")
```

Likewise, we could group countries using `case_when()`. First we use `case_when()` then I'll explain how it works.

```{r}
Births %>% 
  mutate(Country_Group = case_when(Country == "Czechia" ~ "A",
                                   Country == "Spain" ~ "A",
                                   Country == "Belgium" ~ "B",
                                   Country == "Croatia" ~ "B")) %>% 
  group_by(Country_Group, Year) %>% 
  summarize(Births = sum(Births),
            .groups = "drop")
```

### `case_when()`

This helper function is a generalization of `ifelse()` or `if_else()`, as may be familiar from other programs such as Excel. `case_when()` is premised on you being able to delimit all cases given in your data exhaustively. Each case is comma separated and defined in formula notation, where `~` separates a left and a right side. On the left of `~` you define the case with a **logical** statement and on the right side you specify what to assign for that case. By the end of the `case_when()` statement all cases must be handled. Further, cases are handled in the order specified, so where pertinent it makes sense to list cases ordered from specific to general. If there is a most general case meaning something like *everything else*, then you can end `case_when()` with `TRUE ~ 1` (or whatever value you want).

For example, just to demonstrate the concepts, say I have an algorithm where you start with an integer. If the integer is:
 1. divisible by 6 then divide by 2 and add 1
 2. divisible by 3 then multiply by 2
 3. odd add 1
 4. even add 2
 
This is a silly algorithm, I admit. Note only the first condition produces an *odd* result. Note, all integers are handled by conditions 1-4. Note that conditions 3 and 4 handle more cases than conditions 1 and 2. Note also that condition 1 is more specific than 2, because all numbers divisible by 6 are also divisible by 3, but not vice versa. Using `case_when()` and exercising our new modulo skills, an example would be:

```{r}
a <- 1:17
case_when(a %% 6 == 0 ~ a / 2 + 1,
          a %% 3 == 0 ~ a * 2,
          a %% 2 == 1 ~ a + 1,
          a %% 2 == 0 ~ a + 2)
```

If we write the same cases but changing the order of the first two conditions, we see that condition (1) from the initial algorithm is never activated, because divisibility by 3 handles the case earlier.
```{r}
case_when(a %% 3 == 0 ~ a * 2,
          a %% 6 == 0 ~ a / 2 + 1,
          a %% 2 == 1 ~ a + 1,
          a %% 2 == 0 ~ a + 2)
```

### Weighted means

Our main use of `summarize()` today will be for evaluating weighted means. More specifically, we'll calculate the mean age at childbearing.

In general a weighted mean is defined as

$$ \bar{x} = \frac{\sum x_i * w_i}{\sum w_i}$$
For the mean age at childbearing, $x$ is age (exact age at mid-interval we prefer), and $w$ should be either birth counts or age-specific fertility. Since we don't have exposures (yet) to calculate fertility rates, we'll just use raw births by age as the weights.

```{r}
MAB <-
  Births %>% 
  mutate(Age = Age + .5) %>% 
  group_by(Country, Year) %>% 
  summarize(MAB = sum(Births * Age) / sum(Births),
            .groups = "drop") 
glimpse(MAB)
```

While we're here, how about a plot teaser, even though we don't get serious about `ggplot2` until Thursday:
```{r}
MAB %>% 
  ggplot(aes(x = Year, 
             y = MAB, 
             group = Country, 
             color = Country)) +
  geom_line()
```

Allow me to pose a question: All of these lines are increasing. These mean ages are based on observed births in each mother age group, which are a product of fertility rates and population size in each age group. How much of this trend do you suppose is due to changes in age-specific fertility rates versus changes in underlying population structure? To answer this question, we will need to obtain, harmonize, and merge population data to the birth counts data we've been working with. Let's get to it!

# Process population data

Often we get data from different sources that needs to be merged (or joined) into a single merged dataset in order to carry out an analysis. In this case, I've pulled January 1st female population counts data from EUROSTAT, and to make things interesting it's formatted differently and has its own challenges.

\begin{center}
\includegraphics[]{Population_screenshot.png}
\end{center}


## Read in population data

Source: 
[https://appsso.eurostat.ec.europa.eu/nui/show.do?dataset=demo_pjan&lang=en](https://appsso.eurostat.ec.europa.eu/nui/show.do?dataset=demo_pjan&lang=en)

```{r}
Pop <- read_excel("Data/demo_pjan.xlsx",
           range = "A10:CZ510",
           na = ":")
dim(Pop)
```
When we read this in, some rows are entirely `NA` values for population. It will be easier to `filter()` these out a after the population values are stacked in a single column.

## Reformat for joining

To be able to join, we must be able to exactly match on each of our structural criteria: `Country` names, `Year` and `Age` categories.

### Reshape to tidy

```{r}
# check first and last age classes
# colnames(Pop)
Pop <- 
  Pop %>% 
  pivot_longer(`Less than 1 year`:`Unknown`,
               names_to = "AGE",
               values_to = "Population") %>% 
  rename(Country = `GEO/AGE`) %>% 
  filter(!is.na(Population))

glimpse(Pop)
```

### Recode Age classes

What age classes to we have?
```{r}
Pop %>% 
  pull(AGE) %>% 
  unique()
```

Now what would be the easiest way to code this to integer? I'd say: we have special cases for ages 0, 1, the open age group (100) and unknown ages. Every other age follows an exact pattern. Therefore, I propose we treat this with `case_when()` handling all special cases first, then doing some sort of string operation to handle all other cases that follow the pattern `"n years"`. This latter operation could either be a string operation that extracts digits, or a string substitution that deletes `" years"`.

Check:

```{r}
a <- c("10 years","11 years")
# standard
a %>% gsub(pattern = " years", replacement = "") %>% as.integer()
# terse regular expression
a %>% gsub(pattern = "([0-9]+).*$", replacement = "\\1") %>% as.integer()
# or a handy helper function from the readr package:
parse_number(a)
```

Any of these checks would work to handle "everything else" at the end of our `case_when()`

```{r, warning = FALSE}
Pop <-
  Pop %>% 
  mutate(Age = case_when(
    AGE == "Less than 1 year" ~ 0,
    AGE == "Open-ended age class" ~ 100,
    AGE == "Unknown" ~ NA_real_,
    TRUE ~ parse_number(AGE)
  ),
  Year = as.integer(TIME)) %>% 
  select(-TIME, -AGE)
```

### Redistribute unknown ages

Here, rather than rescaling to the stated total as we did for `Births`, we take the other formula that applies the same principle, but framed in terms of redistributing counts with unknown age:

$$\hat{P_x} = P_x + \frac{P_x}{\sum P_x} * P_{Unkown}$$

where the denominator excludes $P_{Unkown}$. Once again, this operation is done inside `mutate()`. Note, we're using `is.na()` three different times as a logical selector! Here, `ifelse()` is used rather than `case_when()` because there is only one condition and it is faster to type out.

```{r}
Pop <-
  Pop %>% 
  # declare groups
  group_by(Country, Year) %>% 
  # 1. move Unknown age up to column
  # 2. replace NAs w 0s in the new UNK column
  mutate(UNK = Population[is.na(Age)],
         UNK = ifelse(is.na(UNK), 0, UNK)) %>%  
  # remove rows with Unknown age
  filter(!is.na(Age)) %>% 
  # do the redistribution
  mutate(Population = Population + Population / sum(Population) * UNK) %>% 
  # remove groups
  ungroup() %>% 
  # remove column no longer needed
  select(-UNK) 
```

## Calculate exposures

Probably we'd rather join exposures to `Births` than January 1st population counts. One final calculating will allow us to introduce a join operation. The approximation we'd like to do is:

$$ Exposure_x = \frac{P^{Jan 1}_x + P^{Dec 31}_x}{2}$$
In other words, just take the average of the population at the start and end of the year. We can approximate the end-of-year population using the following year's January 1st population. Our goal is to do this arithmetic like so `mutate(Exposure = (P1 + P2) / 2)`, so the trick is to create a second `Population` column, consisting in the same `Population` column we already have, but back-dated one year. 

To do this we create a copy of `Pop`, then reduce `Year` by one in that copy, then merge it back to the original `Pop` that we started with. In the process we'll also rename both versions of `Population` to `P1` and `P2` so that we don't get confused. The year-range for `P2` will lose the most recent year, and it will also have one extra year on the lower end, due to the shift. When we **join** the objects together we want to do so only where we have overlapping combinations of `Year` (and `Age` and `Country` need to match too, but these will match exactly in our case).

### Time out for `join` varieties

There are different kinds of joining. Joins have a *left* and *right* side data object. Here are the basic ones, with some example data to make concepts clear:

```{r}
x <- tibble(A = c("a", "b", "c"),
            B = c("t", "u", "v"),
            C = 1:3)
y <- tibble(A = c("a","b","d"),
            B = c("t","u","w"),
            D = 3:1)
x
y
```

1. `left_join()` the left object is primary and the right object is secondary. (left side row count unchanged, but right side could grow or shrink)
```{r}
left_join(x,y)
```

2. `right_join()` the right object is primary and the left object is secondary. (right side row count unchanged, but left side could grow or shrink)
```{r}
right_join(x,y)
```

3. `inner_join()` only keep combinations present in both the left and right. (row count can stay same or shrink)
```{r}
inner_join(x,y)
```

4. `full_join()` keep all combinations (row count can stay same or grow)
```{r}
full_join(x,y)
```

You see in each of these examples that we're politely told in the console which variables were used to determine structural combinations? In these examples, it made good default choices, but in general, we should specify which columns to consider, using the `by` argument:

```{r, eval = FALSE}
left_join(x, y, by = c("A", "B"))
right_join(x, y, by = c("A", "B"))
inner_join(x, y, by = c("A", "B"))
full_join(x, y, by = c("A", "B"))
```

In our case, we want `inner_join(by = c("Country", "Year", "Age))`, make sense?


There are other kinds of joining too! Check out this Rstudio cheat sheet for data reshaping possibilities with `dplyr`: 
[https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf](https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf) Here we're interested in the section called `Combine Tables` on page 2. These cheat sheets are pure gold when you're trying to think through something like this. Now, back to our beloved pipeline!


```{r}
Pop <- Pop %>% 
  # jan 1 of this year = dec 31 of last year
  mutate(Year = Year - 1) %>% 
  # back-dated, this becomes P2 of the previous year
  rename(P2 = Population) %>% 
  # Join together where Year overlaps
  inner_join(Pop, by = c("Country", "Year", "Age")) %>% 
  rename(P1 = Population) %>% 
  # Exposure calc averaging within-age
  mutate(Exposure = (P1 + P2) / 2) 
```

## Bring it all together

The above steps accentuate how designing a processing pipeline happens in stages, and sometimes needs to be mapped out in advance. When doing this sort of work, we always check the results as we go to ensure things are processing as expected. When complete, we can clean up everything into a parsimonious pipeline. This allows you (and others) to think through all the steps in a glance: because tidyverse verbs string together into sentences! We therefore now paste all the above `Pop` processing code into a minimal pipeline:

```{r}
Pop <- read_excel("Data/demo_pjan.xlsx",
           # cell range from visual inspection
           range = "A10:CZ510",
           # NA character also from visual inspection
           na = ":") %>% 
  # stack ages
  pivot_longer(`Less than 1 year`:`Unknown`,
               names_to = "Age",
               values_to = "Population") %>% 
  filter(!is.na(Population)) %>% 
  rename(Country = `GEO/AGE`) %>% 
  # recode age
  mutate(Age = case_when(
    Age == "Less than 1 year" ~ 0,
    Age == "Open-ended age class" ~ 100,
    Age == "Unknown" ~ NA_real_,
    TRUE ~ parse_number(Age)
  ),
  Year = as.integer(TIME)) %>% 
  select(-TIME) %>% 
  # Begin redistribution of pop with unknown age
  group_by(Country, Year) %>% 
  mutate(UNK = Population[is.na(Age)],
         # Not each Country / Year has an Unknown age category
         UNK = ifelse(is.na(UNK), 0, UNK)) %>%  
  filter(!is.na(Age)) %>% 
  # The redistribution (only affects some subsets)
  mutate(Population = Population + Population / sum(Population, na.rm = TRUE) * UNK) %>% 
  select(-UNK) %>% 
  ungroup() 

# Need to cut pipe here because doing a self-join
Pop <-
  Pop %>% 
  # jan 1 of this year = dec 31 of last year
  mutate(Year = Year - 1) %>% 
  rename(P2 = Population) %>% 
  # join together left and right-side pops
  inner_join(Pop, by = c("Country", "Year", "Age")) %>% 
  rename(P1 = Population) %>% 
  # simple exposure calc
  mutate(Exposure = (P1 + P2) / 2) 

```

See how this pipeline is into two pieces? This is because we need to do the self-join part way through to do the exposure calculation.

# Work with merged data

## Join `Pop` and `Births`
Note `Pop` has more `Year` (2012-2019), `Age` (0-100), and `Country` (41) values than does `Births`. However, `Births` has one year that `Pop` does not (2011). If we did `left_join(Pop,Births)` that would be clearly too much. If we did `left_join(Births, Year)` then we'd be closer, but still have an extra year (2011) with no exposure available. Either of these (and by extension a `full_join()`) would still work, but would require extra `filter()` operations in order to get the data down to just the valid combinations of `Country`, `Year`, and `Age`. Hence, we use `inner_join()` again to create our new object, `Dat`.

```{r}
Dat <-
  Births %>% 
  inner_join(Pop, by = c("Country", "Year","Age")) 
```

## Calculate rates

Rate calculation is a straightforward `mutate()` statement. There is no need to apply groups, as age-specific fertility is done row-wise.
```{r}
Dat <-
  Dat %>% 
  mutate(ASFR = Births / Exposure)
```

Now a brief detour to examine the fertility curves and do a quick sanity check that TFR is as expected. A brief explanation: everything inside `aes()` is a *mapping* of our data to coordinate or aesthetic properties. Since `ggplot`s are composed of additive layers, we can keep adding layers using `+`. `geom_line()` is the geometric form that that mapping is translated to. other geometric mappings are also possible. We'll breeze through several other low-key `ggplot` examples before more explicitly explaining things on Thursday.

```{r}
Dat %>% 
  ggplot(aes(x = Age, 
             y = ASFR, 
             group = interaction(Country, Year),
             color = Country,
             alpha = Year)) +
  geom_line()
```

I can't visually integrate those curves, can you? So let's just do a quick check of TFR:

```{r}
Dat %>% 
  group_by(Country, Year) %>% 
  summarize(TFR = sum(ASFR), 
            .groups = "drop") %>% 
  pivot_wider(names_from = Country, values_from = TFR)
```

Full disclosure: When setting up this exercise I at first downloaded Total population by `Country`, `Year`, and `Age`, and I literally didn't realize it until checking the TFRs. They were too small, so I re-downloaded denominators to be sure and that was the problem! Lesson: always do these side checks! If you script is cluttered with this sort of thing, then put them aside in a supplementary script.

## Recalculate mean age at birth using rates

Now we can calculate the MAB using fertility rates rather than birth counts, which ought to reduce the effects of population structure.

```{r}
Dat %>% 
  # age midpoint
  mutate(Age = Age + .5) %>% 
  # independent groups
  group_by(Country, Year) %>% 
  # weighted mean for MAB
  summarize(MAB2 = sum(ASFR * Age) / sum(ASFR),
            .groups = "drop") %>% 
  # join to previous estimate. We do full
  # because year range different, but we can plot everything
  full_join(MAB, by = c("Country","Year")) %>% 
  # stack
  pivot_longer(MAB2:MAB, names_to = "type", values_to = "MAB") %>% 
  # remove NAs from asfr-weighted MAB (no 2011 info)
  filter(!is.na(MAB)) %>% 
  ggplot(aes(x = Year, 
             y = MAB, 
             linetype = type, 
             color = Country, 
             group = interaction(Country,type))) +
  geom_line()
```

From this we see that trends are mostly the same, but not levels, and sometimes slopes are different. One could easily imagine a situation in which `ASFR`-weighted MAB gives a different trend than `Birth`-weighted MAB. One senses Czechia is close this case. Certainly levels can be quite different, and any discrepancy is due to departures from non-uniformity in population structure, which is an odd but precise way of putting it.

# Excercises

1. Choose either rate-weighted MAB or birth-weighted MAB, but redo the calculations in terms of 5-year age groups. Assume that the average age within the interval is simply the midpoint (`Age + 2.5`). Does this change MAB much?

2. The exposure calculation we did could also be framed in terms of a `bind_rows()` followed by a `pivot_wider()` two-step. Can you figure out how to set it up that way? Doing so would produce the same result, and would serve to practice your `dplyr` fluency. Remember, `pivot_wider()` wants `names_from` and `values_from` arguments. `names_from` should be a *new column* specifying whether `Population` refers to `P1` or `P2`, and `values_from` could just be `Population`.

